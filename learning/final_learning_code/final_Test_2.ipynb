{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for trans_image_model\n",
    "import numpy as np\n",
    "from skimage import transform\n",
    "\n",
    "# for face_align\n",
    "from imutils.face_utils import FaceAligner\n",
    "from imutils.face_utils import rect_to_bb\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "# 모델들이 사용할 수 있게 읽어온 이미지를 원하는 array로 변환\n",
    "def trans_image_model(image):\n",
    "    # cv2는 이미지를 읽어올때 GBR로 읽어온다. 이를 RGB로 변환\n",
    "    image_for_model = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_for_model = np.array(image_for_model).astype('float32')/255\n",
    "\n",
    "    # 안경, 선글라스, 마스크 모델용 이미지\n",
    "    # RGB 3채널 + 학습사진크기 : 128 * 128\n",
    "    RGBimage = transform.resize(image_for_model, (128, 128, 3))\n",
    "    RGBimage = np.expand_dims(RGBimage, axis=0)\n",
    "\n",
    "    # 감정 모델용 이미지 \n",
    "    # gray 스케일로 인한 1채널 + 학습사진크기 : 48 * 48 \n",
    "    GRAYimage = transform.resize(image_for_model, (48, 48, 1))\n",
    "    GRAYimage = np.expand_dims(GRAYimage, axis=0)\n",
    "   \n",
    "    return (RGBimage, GRAYimage)\n",
    "\n",
    "\n",
    "def face_align(image):\n",
    "    predictor_model = \"./landmarks.dat\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(predictor_model)\n",
    "    fa = FaceAligner(predictor, desiredFaceWidth=256)\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    rects = detector(gray, 2)\n",
    "\n",
    "    for rect in rects:\n",
    "        (x, y, w, h) = rect_to_bb(rect)\n",
    "        faceOrig = imutils.resize(image[y:y + h, x:x+w], width=256)\n",
    "        faceAligned = fa.align(image, gray, rect)\n",
    "        return faceAligned\n",
    "\n",
    "# 이미지에서 얼굴인식하고 추출\n",
    "def reg_face(image):\n",
    "    \n",
    "    prototxtPath = \"./face_detector/deploy.prototxt\"\n",
    "    weightsPath = \"./face_detector/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "    faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
    "\n",
    "    (h, w) = image.shape[:2]\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0, (224, 224),\n",
    "\t\t(104.0, 177.0, 123.0))\n",
    "    faceNet.setInput(blob)\n",
    "    detections = faceNet.forward()\n",
    "\n",
    "\t# initialize our list of faces, their corresponding locations,\n",
    "\t# and the list of predictions from our face mask network\n",
    "    locs = []\n",
    "\n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with\n",
    "        # the detection\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "\t    # filter out weak detections by ensuring the confidence is\n",
    "\t    # greater than the minimum confidence\n",
    "        if confidence > 0.5:\n",
    "\t        # compute the (x, y)-coordinates of the bounding box for\n",
    "\t        # the object\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\t    \t# ensure the bounding boxes fall within the dimensions of\n",
    "\t    \t# the frame\n",
    "            (startX, startY) = (max(0, startX), max(0, startY))\n",
    "            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "\n",
    "\t\t\t# extract the face ROI, convert it from BGR to RGB channel\n",
    "\t\t\t# ordering, resize it to 224x224, and preprocess it\n",
    "            face = image[startY:endY, startX:endX]\n",
    "            locs.append((startX, startY, endX, endY))\n",
    "    \n",
    "    # only make a predictions if at least one face was detected \n",
    "    if len(locs) > 1:\n",
    "        return -1\n",
    "\n",
    "    return image[locs[0][1]:locs[0][3], locs[0][0]:locs[0][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
    "\n",
    "def emotion():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.load_weights(\"./model/emotion_model.h5\")\n",
    "    return model\n",
    "\n",
    "def mask():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation = 'relu', input_shape = (128, 128, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(64,kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(128,kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adamax', metrics=['accuracy'])\n",
    "    model.load_weights(\"./model/mask_model_ver3.h5\")\n",
    "    return model\n",
    "\n",
    "def glasses():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation = 'relu', input_shape = (128, 128, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(64,kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(128,kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adamax', metrics=['accuracy'])\n",
    "    model.load_weights(\"./model/mask_glasses_model_final3.h5\")\n",
    "    return model\n",
    "\n",
    "def sunglasses():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation = 'relu', input_shape = (128, 128, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(64,kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(128,kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adamax', metrics=['accuracy'])\n",
    "    model.load_weights(\"./model/mask_sunglasses_model_final.h5\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skin():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation = 'relu', input_shape = (128, 128, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(64,kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(128,kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(6, activation = 'softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adamax', metrics=['accuracy'])\n",
    "    model.load_weights(\"./model/skin_model_batch_50_epoch_100.h5\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundrec():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation = 'relu', input_shape = (128, 128, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(64,kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(128,kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adamax', metrics=['accuracy'])\n",
    "    model.load_weights(\"./model/mask_round_rectangle_model_final.h5\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9563279  0.04543835]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "# 학습된 모델 가져오기\n",
    "model_skin = skin()\n",
    "model_glasses = glasses()\n",
    "#model_sunglasses = sunglasses()\n",
    "#model_round = roundrec()\n",
    "model_mask = mask()\n",
    "\n",
    "\n",
    "# 대상 사진파일경로\n",
    "#file_path = '../graduation_dataset/race_test/asian_028.png'\n",
    "#file_path = '../graduation_dataset/race_test/black_023.png'\n",
    "#file_path = '../graduation_dataset/race_test/white_004.png'\n",
    "#file_path = '../graduation_dataset/glasses/test_3/rectangle_002.png'\n",
    "#file_path = '../graduation_dataset/test/black (8).jpg'\n",
    "#file_path = '../graduation_dataset/mask/train4/images/4118.png'\n",
    "#file_path = '../graduation_dataset/mask/train3/nomask_8020.jpg'\n",
    "file_path = '../grauation/face/aaa (2).jpg'\n",
    "# 대상사진\n",
    "image = cv2.imread(file_path)\n",
    "\n",
    "image2 = reg_face(image)\n",
    "\n",
    "RGBimage, GRAYimage = trans_image_model(image2)\n",
    "RGBimage2, GRAYimage2 = trans_image_model(GRAYimage)\n",
    "\n",
    "\n",
    "\n",
    "# cv2.imshow(\"..\", RGBimage[0])\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# #RGBimage[0].show()\n",
    "# print(RGBimage[0])\n",
    "\n",
    "result = model_glasses.predict(RGBimage)\n",
    "\n",
    "# print('예측: ')\n",
    "# if max(result[0]) == result[0][0]:\n",
    "#     print('asian')\n",
    "# elif max(result[0]) == result[0][1]:\n",
    "#     print('black')\n",
    "# else:\n",
    "#     print('white')\n",
    "        \n",
    "# result[0][0] = int(round(result[0][0],3)*10000)\n",
    "# result[0][1] = int(round(result[0][1],3)*10000)\n",
    "# result[0][2] = int(round(result[0][2],3)*10000)\n",
    "# result[0][3] = int(round(result[0][3],3)*10000)\n",
    "# result[0][4] = int(round(result[0][4],3)*10000)\n",
    "# result[0][5] = int(round(result[0][5],3)*10000)\n",
    "            \n",
    "print(result[0])\n",
    "\n",
    "# result_glasses = model_glasses.predict(RGBimage)\n",
    "# if max(result_glasses[0]) == result_glasses[0][0]:\n",
    "#     print('glasses')\n",
    "# else:\n",
    "#     print('noglasses')\n",
    "# print(result_glasses)\n",
    "\n",
    "# # 피부색      -> 피부색에 따라                     \n",
    "# # 마스크      -> no 입     -> mask              => 0 : 입 , 1: 마스크\n",
    "# #           -> no 감정표현 -> 눈 : netural      => default 를 netural로\n",
    "# # 안경       -> 안경추가                        => 0: 안경X , 1: 안경O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
